{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expensive-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "voluntary-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polish-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "del train['target'], train['id']\n",
    "\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "test_id = test['id']\n",
    "del test['id']\n",
    "\n",
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "\n",
    "bin_features = [c for c in train.columns if 'bin' in c]\n",
    "train['bin_sum'] = train[bin_features].sum(axis=1)\n",
    "test['bin_sum'] = test[bin_features].sum(axis=1)\n",
    "\n",
    "features = ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_12_bin', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_11_cat', 'ps_ind_01', 'ps_ind_03', 'ps_ind_15', 'ps_car_11']\n",
    "\n",
    "num_boost_round = 10000\n",
    "params = {\"objective\": \"binary\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.1,\n",
    "          \"num_leaves\": 15,\n",
    "          \"max_bin\": 256,\n",
    "          \"feature_fraction\": 0.6,\n",
    "          \"verbosity\": 0,\n",
    "          \"drop_rate\": 0.1,\n",
    "          \"is_unbalance\": False,\n",
    "          \"max_drop\": 50,\n",
    "          \"min_child_samples\": 10,\n",
    "          \"min_child_weight\": 150,\n",
    "          \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.9,\n",
    "          \"seed\": 2018,\n",
    "          \"force_row_wise\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "executive-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "kf = kfold.split(train, train_label)\n",
    "\n",
    "cv_train = np.zeros(len(train_label))\n",
    "cv_pred = np.zeros(len(test_id))    \n",
    "best_trees = []\n",
    "fold_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organic-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n",
    "    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n",
    "\n",
    "    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "\n",
    "    return G_pred * 1. / G_true\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', Gini(labels, preds), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coated-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.151538\tvalid_0's gini: 0.29314\n",
      "0.29314009182258344\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's binary_logloss: 0.152349\tvalid_0's gini: 0.265321\n",
      "0.2653211692773015\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.152092\tvalid_0's gini: 0.273312\n",
      "0.2733117835288082\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's binary_logloss: 0.151972\tvalid_0's gini: 0.280085\n",
      "0.28008456208849136\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid_0's binary_logloss: 0.151568\tvalid_0's gini: 0.286495\n",
      "0.2864950254393784\n"
     ]
    }
   ],
   "source": [
    "for i, (train_fold, validate) in enumerate(kf):\n",
    "    X_train, X_validate, label_train, label_validate = train.loc[train_fold, :], train.loc[validate, :], train_label[train_fold], train_label[validate]\n",
    "    \n",
    "    for feature in features:\n",
    "        map_dic = pd.DataFrame([X_train[feature], label_train]).T.groupby(feature).agg('mean')\n",
    "        map_dic = map_dic.to_dict()['target']\n",
    "        X_train[feature + '_target_en'] = X_train[feature].apply(lambda x: map_dic.get(x, 0))\n",
    "        X_validate[feature + '_target_enc'] = X_validate[feature].apply(lambda x: map_dic.get(x, 0))\n",
    "        test[feature + '_target_enc'] = test[feature].apply(lambda x: map_dic.get(x, 0))\n",
    "\n",
    "    dtrain = lgbm.Dataset(X_train, label_train)\n",
    "    dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "\n",
    "    verbose_eval = 0\n",
    "    \n",
    "    bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, feval=evalerror, \n",
    "                     callbacks=[lgbm.early_stopping(stopping_rounds=100, verbose=True), lgbm.log_evaluation(verbose_eval)])\n",
    "    \n",
    "    bst.save_model('./model/baseline_model_{}.txt'.format(i))\n",
    "    best_trees.append(bst.best_iteration)\n",
    "    cv_pred += bst.predict(test, num_iteration=bst.best_iteration)\n",
    "    cv_train[validate] += bst.predict(X_validate)\n",
    "\n",
    "    score = Gini(label_validate, cv_train[validate])\n",
    "    print(score)\n",
    "    fold_scores.append(score)\n",
    "\n",
    "cv_pred /= NFOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bibliographic-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score:\n",
      "0.27972510469464895\n",
      "[0.29314009182258344, 0.2653211692773015, 0.2733117835288082, 0.28008456208849136, 0.2864950254393784]\n",
      "[204, 83, 125, 201, 99] 142.4\n"
     ]
    }
   ],
   "source": [
    "print(\"cv score:\")\n",
    "print(Gini(train_label, cv_train))\n",
    "print(fold_scores)\n",
    "print(best_trees, np.mean(best_trees))\n",
    "\n",
    "pd.DataFrame({'id': test_id, 'target': cv_pred}).to_csv('./lgbm_baseline.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-norwegian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
