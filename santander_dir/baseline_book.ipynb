{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "commercial-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from mapk import mapk\n",
    "import pickle\n",
    "\n",
    "\n",
    "from process_data_santander import clean_data, data_engineering, split_data\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "employed-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "prods = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1', 'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n",
    "             'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
    "             'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1', 'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n",
    "             'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1', 'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n",
    "\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "\n",
    "train_path = '../data/santander/train_ver2.csv'\n",
    "test_path = '../data/santander/test_ver2.csv'\n",
    "\n",
    "# use_dates = ['2015-01-28', '2015-02-28', '2015-03-28', '2015-04-28',\n",
    "#        '2015-05-28', '2015-06-28', '2015-07-28', '2015-08-28',\n",
    "#        '2015-09-28', '2015-10-28', '2015-11-28', '2015-12-28',\n",
    "#        '2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28',\n",
    "#        '2016-05-28']\n",
    "\n",
    "# test_date =  '2016-06-28'\n",
    "\n",
    "\n",
    "use_dates = ['2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "test_date =  '2016-06-28'\n",
    "\n",
    "param = {\n",
    "        'booster': 'gbtree',\n",
    "        'max_depth': 8,\n",
    "        'nthread': 4,\n",
    "        'num_class': 24,\n",
    "        'objective': 'multi:softprob',\n",
    "        'silent': 1,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'eta': 0.1,\n",
    "        'min_child_weight': 10,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'colsample_bylevel': 0.9,\n",
    "        'seed': 2018,\n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Phase 1 ####\n",
    "# DADA LAODING. \n",
    "# Preferably in an SQL server or a distributed storage. \n",
    "print(\"Loading data from csv files\")\n",
    "trn = pd.read_csv(train_path, low_memory=False)\n",
    "tst = pd.read_csv(test_path, low_memory=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Phase 2 ####\n",
    "# DATA CLEANING. \n",
    "# Preferably completed before SQL server. \n",
    "print(\"Cleaning and joining training data.\")\n",
    "df = clean_data(trn, tst)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Phase 3 ####\n",
    "# Data engineering. \n",
    "# Preferably implemented with SQL or Apache Spark.\n",
    "# Preferably saved to a feature store. \n",
    "print(\"Cleaning and joining training data.\")\n",
    "trn, tst, features = data_engineering(df, use_dates, test_date)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    ↑ DATA ENGINEERING\n",
    "    \n",
    "    Feature store should be placed here. \n",
    "    *A border between data engineering and data scinece. \n",
    "    \n",
    "    ↓ DATA SCIENCE\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Phase 4 ####\n",
    "# DATA SPLIT FOR CROSS VALIDATION. \n",
    "print(\"Splitting into training and validation data as Numpy arrays..\")\n",
    "X_trn, Y_trn, X_vld, Y_vld = split_data(trn, features)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Phase 5 ####\n",
    "# MODEL INITIALIZATION AND TRAINING. \n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "print(\"Training a model\")\n",
    "model = xgb.train(param, dtrn, num_boost_round=10, evals=watch_list, early_stopping_rounds=20)\n",
    "    \n",
    "pickle.dump(model, open(\"./model/xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit\n",
    "\n",
    "#dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "#best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "    \n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Phase 5 ####\n",
    "# INFERENCE AND EVAULATION    \n",
    "print(\"Inference on test data\")    \n",
    "X_tst = tst[features].values\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst['ncodpers'].values\n",
    "#preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
    "preds_tst = preds_tst - tst[[prod + '_prev' for prod in prods]].values\n",
    "    \n",
    "print(\"Exporting results on test data\")\n",
    "submit_file = open('./model/xgb.baseline.2015-06-28', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
